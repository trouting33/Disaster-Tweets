{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\w2673\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\w2673\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import nltk, re, string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, GlobalAveragePooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "tf.config.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "import tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "with open('train.csv', 'rb') as f:\n",
    "    train_df = pd.read_csv(f)\n",
    "with open('test.csv', 'rb') as f:\n",
    "    test_df = pd.read_csv(f)\n",
    "# train_df = pd.read_csv('train.csv', encoding=\"ISO-8859-1\")\n",
    "# test_df = pd.read_csv('test.csv', encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7613, 5), (3263, 4))"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train and text shapes\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 110 tweets duplicated in train_df.\n"
     ]
    }
   ],
   "source": [
    "# Check the duplicated tweets\n",
    "dup_train = train_df['text'].duplicated().sum()\n",
    "print(f'there are {dup_train} tweets duplicated in train_df.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplictes\n",
    "train_df = train_df.drop_duplicates(subset=['text'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7503, 5), (3263, 4))"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new shape for train data\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4305\n",
       "1    3198\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check the distribution of the disaster and no-disaster tweets\n",
    "count = train_df['target'].value_counts()\n",
    "sns.barplot(count.index, count)\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the data and removing the stopwords\n",
    "def Data_Cleaning(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"won\\'t\", \"will not\", text)\n",
    "    text = re.sub(\"can\\'t\", \"can not\", text)\n",
    "    text = re.sub(\"don\\'t\", \"do not\", text)\n",
    "    \n",
    "    text = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+',' ', text)\n",
    "    text = re.sub(r'&amp?;',' ', text)\n",
    "    text = re.sub(r'&lt;',' ', text)\n",
    "    text = re.sub(r'&gt;',' ', text)\n",
    "    \n",
    "    text = re.sub(r'\\d{2}:\\d{2}:\\d{2}', ' ', text)\n",
    "    text = re.sub(r'UTC', ' ', text)\n",
    "    text = re.sub(r'\\d{2}km', ' ', text)\n",
    "    text = re.sub(r\"\\b\\d+\\b\", \" \", text) # removing the numbers\n",
    "\n",
    "    text = re.sub(r\"#\",\"\",text) \n",
    "    text = re.sub(r\"(?:\\@)\\w+\", ' ', text)\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    \n",
    "    text = text.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation)))\n",
    "    text = re.sub(' +', ' ', text) # remove multiple spaces\n",
    "    \n",
    "    text = [word for word in word_tokenize(text) if not word in stopwords.words('english')]\n",
    "    text = ' '.join(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the cleaning function to the dataset and creating a new column of the cleaned data\n",
    "train_df['cleaned'] = train_df['text'].apply(lambda x: Data_Cleaning(x))\n",
    "test_df['cleaned'] = test_df['text'].apply(lambda x: Data_Cleaning(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7598</th>\n",
       "      <td>10853</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Father-of-three Lost Control of Car After Over...</td>\n",
       "      <td>1</td>\n",
       "      <td>father three lost control car overtaking colli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7599</th>\n",
       "      <td>10854</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.3 #Earthquake in 9Km Ssw Of Anza California ...</td>\n",
       "      <td>1</td>\n",
       "      <td>earthquake 9km ssw anza california iphone user...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7601</th>\n",
       "      <td>10859</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#breaking #LA Refugio oil spill may have been ...</td>\n",
       "      <td>1</td>\n",
       "      <td>breaking la refugio oil spill may costlier big...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7602</th>\n",
       "      <td>10860</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a siren just went off and it wasn't the Forney...</td>\n",
       "      <td>1</td>\n",
       "      <td>siren went forney tornado warning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7603</th>\n",
       "      <td>10862</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Officials say a quarantine is in place at an A...</td>\n",
       "      <td>1</td>\n",
       "      <td>officials say quarantine place alabama home po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7604</th>\n",
       "      <td>10863</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#WorldNews Fallen powerlines on G:link tram: U...</td>\n",
       "      <td>1</td>\n",
       "      <td>worldnews fallen powerlines g link tram update...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7605</th>\n",
       "      <td>10864</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>on the flip side I'm at Walmart and there is a...</td>\n",
       "      <td>1</td>\n",
       "      <td>flip side walmart bomb everyone evacuate stay ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7606</th>\n",
       "      <td>10866</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Suicide bomber kills 15 in Saudi security site...</td>\n",
       "      <td>1</td>\n",
       "      <td>suicide bomber kills saudi security site mosqu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "      <td>two giant cranes holding bridge collapse nearb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "      <td>latest homes razed northern california wildfir...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "7598  10853     NaN      NaN   \n",
       "7599  10854     NaN      NaN   \n",
       "7601  10859     NaN      NaN   \n",
       "7602  10860     NaN      NaN   \n",
       "7603  10862     NaN      NaN   \n",
       "7604  10863     NaN      NaN   \n",
       "7605  10864     NaN      NaN   \n",
       "7606  10866     NaN      NaN   \n",
       "7608  10869     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \\\n",
       "7598  Father-of-three Lost Control of Car After Over...       1   \n",
       "7599  1.3 #Earthquake in 9Km Ssw Of Anza California ...       1   \n",
       "7601  #breaking #LA Refugio oil spill may have been ...       1   \n",
       "7602  a siren just went off and it wasn't the Forney...       1   \n",
       "7603  Officials say a quarantine is in place at an A...       1   \n",
       "7604  #WorldNews Fallen powerlines on G:link tram: U...       1   \n",
       "7605  on the flip side I'm at Walmart and there is a...       1   \n",
       "7606  Suicide bomber kills 15 in Saudi security site...       1   \n",
       "7608  Two giant cranes holding a bridge collapse int...       1   \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1   \n",
       "\n",
       "                                                cleaned  \n",
       "7598  father three lost control car overtaking colli...  \n",
       "7599  earthquake 9km ssw anza california iphone user...  \n",
       "7601  breaking la refugio oil spill may costlier big...  \n",
       "7602                  siren went forney tornado warning  \n",
       "7603  officials say quarantine place alabama home po...  \n",
       "7604  worldnews fallen powerlines g link tram update...  \n",
       "7605  flip side walmart bomb everyone evacuate stay ...  \n",
       "7606  suicide bomber kills saudi security site mosqu...  \n",
       "7608  two giant cranes holding bridge collapse nearb...  \n",
       "7612  latest homes razed northern california wildfir...  "
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>happened terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>heard earthquake different cities stay safe ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>forest fire spot pond geese fleeing across str...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>apocalypse lighting spokane wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>typhoon soudelor kills china taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash   \n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...   \n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...   \n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires   \n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan   \n",
       "\n",
       "                                             cleaned  \n",
       "0                        happened terrible car crash  \n",
       "1  heard earthquake different cities stay safe ev...  \n",
       "2  forest fire spot pond geese fleeing across str...  \n",
       "3              apocalypse lighting spokane wildfires  \n",
       "4                typhoon soudelor kills china taiwan  "
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_encode(texts, tokenizer, max_len=512):\n",
    "    all_tokens = []\n",
    "    masks = []\n",
    "    segments = []\n",
    "    \n",
    "    for text in texts:\n",
    "        text = tokenizer.tokenize(text)\n",
    "            \n",
    "        text = text[:max_len-2]\n",
    "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "        pad_len = max_len - len(input_sequence)\n",
    "        \n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "        tokens += [0] * pad_len\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_len\n",
    "        \n",
    "        all_tokens.append(tokens)\n",
    "        masks.append(pad_masks)\n",
    "        segments.append(segment_ids)\n",
    "    \n",
    "    return np.array(all_tokens), np.array(masks), np.array(segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer from tokenization script\n",
    "F_tokenizer = tokenization.FullTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1'\n",
    "hub_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1'\n",
    "bert_layer = hub.KerasLayer(hub_url, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "vocabulary = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "\n",
    "tokenizer = F_tokenizer(vocabulary, to_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(bert_layer, max_len=512):\n",
    "    \n",
    "    input_word_ids = Input(shape = (max_len,), dtype = tf.int32, name = \"input_word_ids\")\n",
    "    input_mask = Input(shape = (max_len,), dtype = tf.int32, name = \"input_mask\")\n",
    "    segment_ids = Input(shape = (max_len,), dtype = tf.int32, name = \"segment_ids\")\n",
    "\n",
    "    pooled_sequence, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "    clf_output = sequence_output[:, 0, :]\n",
    "    output = Dense(1, activation='sigmoid')(clf_output)\n",
    "    \n",
    "    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs = output)\n",
    "    model.compile(Adam(lr=1e-5), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([train_df, test_df], axis=0).reset_index(drop=True)\n",
    "max_length = len(max(df.cleaned, key=len))\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = bert_encode(train_df.cleaned.values, tokenizer, max_len=140)\n",
    "test_input = bert_encode(test_df.cleaned.values, tokenizer, max_len=140)\n",
    "train_labels = train_df['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_word_ids (InputLayer)     [(None, 140)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 140)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 140)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_2 (KerasLayer)      [(None, 768), (None, 109482241   input_word_ids[0][0]             \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem (Slici (None, 768)          0           keras_layer_2[0][1]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            769         tf.__operators__.getitem[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 109,483,010\n",
      "Trainable params: 109,483,009\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(bert_layer, max_len=140)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "352/352 [==============================] - 4721s 13s/step - loss: 0.4475 - accuracy: 0.8015 - val_loss: 0.4019 - val_accuracy: 0.8273\n",
      "Epoch 2/5\n",
      "352/352 [==============================] - 4247s 12s/step - loss: 0.2904 - accuracy: 0.8802 - val_loss: 0.4411 - val_accuracy: 0.8156\n",
      "Epoch 3/5\n",
      "352/352 [==============================] - 4285s 12s/step - loss: 0.1369 - accuracy: 0.9508 - val_loss: 0.4935 - val_accuracy: 0.8161\n",
      "Epoch 4/5\n",
      "352/352 [==============================] - 4692s 13s/step - loss: 0.0724 - accuracy: 0.9735 - val_loss: 0.6928 - val_accuracy: 0.7900\n",
      "Epoch 5/5\n",
      "352/352 [==============================] - 4699s 13s/step - loss: 0.0476 - accuracy: 0.9799 - val_loss: 0.7194 - val_accuracy: 0.8134\n"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint('model.h5', monitor = 'val_loss', save_best_only = True)\n",
    "train_history = model.fit(train_input, train_labels, validation_split = 0.25, epochs = 5, callbacks = [checkpoint], batch_size = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model.h5')\n",
    "test_pred = model.predict(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['target'] = test_pred.round().astype(int)\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
